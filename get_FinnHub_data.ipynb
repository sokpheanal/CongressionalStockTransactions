{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducible steps to create a corpus from [FinnHub](https://finnhub.io/).\n",
    "\n",
    "# Pseudocode\n",
    "\n",
    "Below is a list of the steps we take.\n",
    "Keep in mind that these steps are a 10 thousand foot view.\n",
    "The implementation will be commented to a more detailed level.\n",
    "\n",
    "1. Get the tickers from the [SEC](https://www.sec.gov/file/company-tickers)\n",
    "2. Using the retrieved data, get the tickers for every publicly traded stock in the U.S. market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import finnhub\n",
    "import time\n",
    "\n",
    "from finnhub.exceptions import FinnhubAPIException\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv, dotenv_values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_url = 'https://www.sec.gov/files/company_tickers.json'\n",
    "user_agent = 'FinnHub-Data-Ingestion'\n",
    "limit = 20\n",
    "\n",
    "data_folder = Path('./data/')\n",
    "tickers_file = data_folder.joinpath('./tickers.csv')\n",
    "raw_folder = data_folder.joinpath('./raw/')\n",
    "raw_json_folder = raw_folder.joinpath('./json_folder/')\n",
    "raw_csv_folder = raw_folder.joinpath('./csv_folder/')\n",
    "corpus_folder = data_folder.joinpath('./corpus/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "\n",
    "1. Get the list of tickers from the SEC\n",
    "2. Convert the tickers into an array, then sort it.\n",
    "3. Save the tickers to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tickers(tickers_file: Path, tickers_url: str, user_agent: str, ) -> pd.DataFrame:\n",
    "    if not tickers_file.exists():\n",
    "        tickers = None\n",
    "        with requests.Session() as session:\n",
    "            session.headers['User-Agent'] = user_agent\n",
    "            with session.get(tickers_url) as result:\n",
    "                if result.status_code == 200:\n",
    "                    t1 = json.loads(result.text)\n",
    "                    t2 = [x for x in t1.values()]\n",
    "                    t3 = sorted(t2, key = lambda tup: tup['ticker'])\n",
    "                    tickers = [(x['cik_str'], x['ticker'], x['title']) for x in t3]\n",
    "        if tickers is not None:\n",
    "            df = pd.DataFrame(tickers, columns = ['CIK', 'Ticker', 'Name'])\n",
    "            if not tickers_file.parent.exists():\n",
    "                tickers_file.parent.mkdir(parents = True)\n",
    "            df.to_csv(tickers_file, index = False)\n",
    "        else:\n",
    "            raise RuntimeError('Error retrieving tickers')          \n",
    "    return pd.read_csv(tickers_file) #type: ignore\n",
    "\n",
    "tickers_df = get_tickers(tickers_file, tickers_url, user_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "\n",
    "1. Iterate through each ticker in the `tickers.csv` file and download all available trading data.\n",
    "2. Save data of each ticker to JSON files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize API variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "base_url = '/api/v1'\n",
    "\n",
    "tickers = tickers_df['Ticker']\n",
    "\n",
    "finnhub_client = finnhub.Client(api_key=os.getenv(\"API_KEY\"))\n",
    "\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2023-12-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Download trading data in JSON format from API and save to data/raw folder with filename {ticker}.json*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_json_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for ticker in tqdm(tickers):\n",
    "    try:\n",
    "        # Fetch congressional trading data for the specified time frame\n",
    "        response = finnhub_client.congressional_trading(ticker, start_date, end_date)\n",
    "\n",
    "        if 'data' in response and response['data']:\n",
    "            filename = raw_json_folder.joinpath(f'{ticker}.json')\n",
    "            with open(filename, 'w') as jsonfile:\n",
    "                json.dump(response, jsonfile)\n",
    "                print(f'Response for {ticker} saved to {filename}')\n",
    "        else:\n",
    "            print(f'No trade data available for {ticker}. Skipping...')\n",
    "    except FinnhubAPIException as e:\n",
    "        print(f'Skipping {ticker}: {e}')\n",
    "\n",
    "    # Throttle the API calls to stay within the rate limit\n",
    "    time.sleep(1/6) \n",
    "\n",
    "print(\"Data download completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "*Converting JSON files csv and merge all csv files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_folder.mkdir(parents=True, exist_ok=True)\n",
    "raw_csv_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "json_files = list(raw_json_folder.glob('*.json'))\n",
    "\n",
    "concatenated_data = pd.DataFrame()\n",
    "\n",
    "for json_file in tqdm(json_files):\n",
    "    try:\n",
    "        # Load JSON data from file\n",
    "        with open(json_file, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        # Convert JSON data to DataFrame\n",
    "        data = pd.DataFrame(json_data['data'])\n",
    "\n",
    "        # Append DataFrame to concatenated_data\n",
    "        concatenated_data = pd.concat([concatenated_data, data], ignore_index=True)\n",
    "\n",
    "        # Construct CSV file path\n",
    "        csv_file = raw_csv_folder.joinpath(json_file.stem + '.csv')\n",
    "\n",
    "        # Write DataFrame to CSV file\n",
    "        data.to_csv(csv_file, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {json_file}: {e}\")\n",
    "\n",
    "# Write concatenated data to a single CSV file\n",
    "concatenated_csv_file = corpus_folder.joinpath('all_available_transactions.csv')\n",
    "concatenated_data.to_csv(concatenated_csv_file, index=False)\n",
    "\n",
    "print(\"All JSON files converted to CSV and concatenated into one big CSV file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
